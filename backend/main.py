import os
import sys

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import shutil
import json
import re
import torch
import uvicorn
import logging
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import pipeline
from deep_translator import GoogleTranslator

logging.getLogger("transformers").setLevel(logging.ERROR)

# ==========================================
# 1. è·¯å¾„ä¸ç¯å¢ƒé…ç½®
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)

if project_root not in sys.path:
    sys.path.append(project_root)

UPLOAD_DIR = os.path.join(current_dir, "uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)

print("[Backend] æ­£åœ¨å¯¼å…¥æ ¸å¿ƒæ¨¡å—...")
try:
    from main_core import MultimodalSystem
    from src.llm.service import LLMService
    from src.utils.prompts import get_system_prompt, PERSONA_CONFIG, GLOBAL_INSTRUCTIONS
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# ==========================================
# 2. åˆå§‹åŒ– FastAPI åº”ç”¨
# ==========================================
app = FastAPI(title="EmoChat Pro API", description="å¤šæ¨¡æ€æƒ…æ„Ÿäº¤äº’åç«¯æ¥å£")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

core_system = None
llm_service = None
stt_pipe = None


@app.on_event("startup")
async def startup_event():
    global core_system, llm_service, stt_pipe
    print("\n[Backend] æœåŠ¡å™¨å¯åŠ¨ä¸­ï¼Œæ­£åœ¨åŠ è½½æ¨¡å‹...")

    core_system = MultimodalSystem()
    llm_service = LLMService()

    print("[System] åŠ è½½ Whisper STT...")
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    try:
        stt_pipe = pipeline("automatic-speech-recognition", model="openai/whisper-small", device=device)
    except Exception as e:
        print(f"âš ï¸ Whisper é™çº§ä¸º CPU: {e}")
        stt_pipe = pipeline("automatic-speech-recognition", model="openai/whisper-small", device="cpu")

    print("âœ… [Backend] æœåŠ¡å·²å°±ç»ªï¼\n")


# ==========================================
# 3. è¾…åŠ©å‡½æ•°
# ==========================================
def contains_chinese(text):
    return bool(re.search(r'[\u4e00-\u9fa5]', text))


def translate_to_english(text):
    try:
        if not text or not text.strip(): return ""
        translated = GoogleTranslator(source='auto', target='en').translate(text)
        # print(f"ğŸ”¤ [Translation] {text} -> {translated}")
        return translated
    except Exception:
        return text


# ==========================================
# 4. API æ¥å£
# ==========================================

@app.get("/")
async def root():
    return {"message": "EmoChat Pro Backend is Running!"}


# --- æ¥å£ A: ç”Ÿæˆå¼€åœºç™½ ---
class GreetingRequest(BaseModel):
    mode: str
    custom_role: str = ""


@app.post("/api/greeting")
async def generate_greeting(request: GreetingRequest):
    try:
        if request.mode == "è‡ªå®šä¹‰" or request.mode == "è‡ªå®šä¹‰æ™ºèƒ½ä½“":
            role_def = f"ä½ æ‰®æ¼”ï¼š{request.custom_role}" if request.custom_role else "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"
            intro_sys_prompt = f"{role_def}ã€‚\n{GLOBAL_INSTRUCTIONS}\nè¯·æ ¹æ®ä½ çš„äººè®¾ï¼Œå‘ç”¨æˆ·åšä¸ªè‡ªæˆ‘ä»‹ç»å¹¶å¼€å¯è¯é¢˜ã€‚"
        else:
            config = PERSONA_CONFIG.get(request.mode, PERSONA_CONFIG["æ—¥å¸¸é—²èŠ"])
            intro_sys_prompt = f"{config['role_def']}\n{GLOBAL_INSTRUCTIONS}\nè¯·æ ¹æ®ä½ çš„äººè®¾ï¼Œç”¨æœ€ç¬¦åˆä½ é£æ ¼çš„æ–¹å¼å‘ç”¨æˆ·æ‰“æ‹›å‘¼å¹¶å¼€å¯è¯é¢˜ã€‚"

        messages = [{"role": "system", "content": intro_sys_prompt}]
        reply = llm_service.chat(messages, temperature=0.8)

        return {"status": "success", "reply": reply}
    except Exception as e:
        print(f"âŒ [Error] Greeting: {e}")
        return {"status": "error", "message": str(e)}


# --- æ¥å£ B: çº¯è¯­éŸ³è½¬æ–‡å­— (STT) ---
@app.post("/api/stt")
async def speech_to_text(audio: UploadFile = File(...)):
    try:
        audio_path = os.path.join(UPLOAD_DIR, f"temp_stt_{audio.filename}")
        with open(audio_path, "wb") as buffer:
            shutil.copyfileobj(audio.file, buffer)

        stt_res = stt_pipe(audio_path, generate_kwargs={"language": "chinese"})

        return {"status": "success", "text": stt_res["text"]}
    except Exception as e:
        return {"status": "error", "message": str(e)}


# --- æ¥å£ C: å…¨æ¨¡æ€åˆ†æ (æ¥æ”¶å‰ç«¯ä¼ æ¥çš„æœ€ä½³ç…§ç‰‡) ---
@app.post("/api/analyze")
async def analyze_sentiment(
        audio: UploadFile = File(...),
        image: UploadFile = File(None),
        text: str = Form(""),
        mode: str = Form("æ—¥å¸¸é—²èŠ"),
        custom_role: str = Form(""),
        history: str = Form("[]")
):
    try:
        # 1. ä¿å­˜éŸ³é¢‘
        audio_path = os.path.join(UPLOAD_DIR, f"temp_{audio.filename}")
        with open(audio_path, "wb") as buffer:
            shutil.copyfileobj(audio.file, buffer)

        # 2. ä¿å­˜å›¾ç‰‡
        image_path = None
        if image:
            image_path = os.path.join(UPLOAD_DIR, f"temp_{image.filename}")
            with open(image_path, "wb") as buffer:
                shutil.copyfileobj(image.file, buffer)

        # 3. æ ¸å¿ƒåˆ†æ
        text_for_model = text
        if contains_chinese(text):
            text_for_model = translate_to_english(text)

        analysis_res = core_system.analyze(
            audio_path=audio_path,
            image_path=image_path,
            text_content=text_for_model
        )

        final_emotion = analysis_res['final_decision'].upper()
        fused_scores = analysis_res['details']['fused_scores']
        confidence = max(fused_scores.values()) if fused_scores else 0.0

        # 4. LLM ç”Ÿæˆ
        sys_prompt = get_system_prompt(mode, final_emotion, custom_role)
        try:
            client_history = json.loads(history)
        except:
            client_history = []

        messages = [{"role": "system", "content": sys_prompt}]
        messages.extend(client_history)
        messages.append({"role": "user", "content": text})

        ai_reply = llm_service.chat(messages)

        return {
            "status": "success",
            "data": {
                "text": text,
                "emotion": final_emotion,
                "confidence": confidence,
                "reply": ai_reply,
                "scores": fused_scores,
                "vision_score": analysis_res['details']['vision'],
                "audio_score": analysis_res['details']['audio'],
                "text_score": analysis_res['details']['text']
            }
        }

    except Exception as e:
        print(f"âŒ [Error] Analyze: {e}")
        return {"status": "error", "message": str(e)}


# --- æ¥å£ D: å®æ—¶è§†è§‰æµ  ---
import cv2
import numpy as np


@app.post("/api/live_vision")
async def live_vision_analysis(image: UploadFile = File(...)):
    """
    ä¸åšæ—¥å¿—è¾“å‡ºä»¥é˜²åˆ·å±
    """
    try:
        contents = await image.read()
        nparr = np.frombuffer(contents, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        result = core_system.vision_engine.detect_face_and_emotion(img)

        if result:
            return {"status": "success", "data": result}
        else:
            return {"status": "empty", "message": "No face"}

    except Exception:
        return {"status": "error"}


if __name__ == "__main__":
    # log_level="warning"å‡å°‘æ—¥å¿—
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False, log_level="warning")