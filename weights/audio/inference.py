import os
import sys
import numpy as np
import librosa
import torch
import torch.nn.functional as F

# ==========================================
# 1. è·¯å¾„ä¿®å¤ä¸ç¯å¢ƒè®¾ç½®
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__))

if current_dir not in sys.path:
    sys.path.append(current_dir)

try:
    from config import AudioConfig, ModelConfig, EMOTION_LABELS
    from .models.acrnn_v3 import ACRNNv3
except ImportError as e:
    print(f"âŒ [Audio] å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    from src.modules.audio.config import AudioConfig, ModelConfig, EMOTION_LABELS
    from src.modules.audio.models.acrnn_v3 import ACRNNv3


class EmotionPredictor:
    def __init__(self, model_path=None, device=None):
        """
        åˆå§‹åŒ–éŸ³é¢‘æƒ…æ„Ÿé¢„æµ‹å™¨
        :param model_path: æƒé‡æ–‡ä»¶è·¯å¾„ (é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾ weights/audio/...)
        :param device: 'cuda' or 'cpu'
        """
        # 1. é…ç½®
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)

        # 2. æ™ºèƒ½è·¯å¾„æŸ¥æ‰¾
        if model_path is None:

            project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
            default_weights = os.path.join(project_root, "weights", "audio", "best_acrnn_v3_1.pth")

            if os.path.exists(default_weights):
                model_path = default_weights
            else:
                # å¤‡ç”¨æ–¹æ¡ˆ
                local_weights = os.path.join(current_dir, "best_acrnn_v3_1.pth")
                if os.path.exists(local_weights):
                    model_path = local_weights
                else:
                    raise FileNotFoundError(
                        f"âŒ æ— æ³•è‡ªåŠ¨å®šä½æƒé‡æ–‡ä»¶ï¼\n"
                        f"è¯·æ£€æŸ¥è·¯å¾„: {default_weights}\n"
                        f"æˆ–è€…åœ¨åˆå§‹åŒ–æ—¶æ‰‹åŠ¨ä¼ å…¥ model_path å‚æ•°ã€‚"
                    )

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æŒ‡å®šçš„æƒé‡æ–‡ä»¶: {model_path}")

        print(f"ğŸ§  [Audio] åŠ è½½æ¨¡å‹æƒé‡: {os.path.basename(model_path)}")

        # 3. åŠ è½½é…ç½®ä¸æ¨¡å‹æ¶æ„
        self.audio_cfg = AudioConfig()
        self.model_cfg = ModelConfig()

        self.model = ACRNNv3(config=self.model_cfg).to(self.device)

        # 4. åŠ è½½æƒé‡
        try:
            state_dict = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            self.model.eval()
            # print("âœ… éŸ³é¢‘æ¨¡å‹åŠ è½½å°±ç»ª")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

    def _preprocess_single(self, audio_path):
        try:
            # A. Load & Resample
            y, sr = librosa.load(audio_path, sr=self.audio_cfg.sample_rate)

            # B. Trim Silence
            y, _ = librosa.effects.trim(y)

            # C. Pad/Truncate
            target_len = int(self.audio_cfg.sample_rate * self.audio_cfg.duration)
            if len(y) > target_len:
                y = y[:target_len]
            else:
                padding = target_len - len(y)
                y = np.pad(y, (0, padding), mode='constant')

            # D. Log-Mel Spectrogram
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr,
                n_mels=self.audio_cfg.n_mels,
                n_fft=self.audio_cfg.n_fft,
                hop_length=self.audio_cfg.hop_length
            )
            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)

            # E. Fix Time Steps (Width=130)
            target_width = self.audio_cfg.max_time_steps
            if log_mel_spec.shape[1] > target_width:
                log_mel_spec = log_mel_spec[:, :target_width]
            else:
                padding = target_width - log_mel_spec.shape[1]
                log_mel_spec = np.pad(log_mel_spec, ((0, 0), (0, padding)), mode='constant')

            # F. To Tensor (Batch, Channel, Freq, Time)
            spec_tensor = torch.tensor(log_mel_spec, dtype=torch.float32)
            spec_tensor = spec_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, 128, 130)
            return spec_tensor.to(self.device)

        except Exception as e:
            print(f"âŒ éŸ³é¢‘é¢„å¤„ç†é”™è¯¯: {e}")
            return None

    def predict(self, audio_path):
        """
        :return: {'happy': 0.85, 'sad': 0.05, ...} (æŒ‰æ¦‚ç‡é™åº)
        """
        if not os.path.exists(audio_path):
            return {"error": f"Audio file not found: {audio_path}"}

        # 1. é¢„å¤„ç†
        tensor = self._preprocess_single(audio_path)
        if tensor is None:
            return {"error": "Audio processing failed"}

        # 2. æ¨ç†
        with torch.no_grad():
            logits = self.model(tensor)
            probs = F.softmax(logits, dim=1).cpu().numpy()[0]

        # 3. æ ¼å¼åŒ–ç»“æœ
        result = {label: float(prob) for label, prob in zip(EMOTION_LABELS, probs)}

        # 4. æ¦‚ç‡é™åºæ’åˆ—
        sorted_result = dict(sorted(result.items(), key=lambda x: x[1], reverse=True))
        return sorted_result


# --- ç‹¬ç«‹æµ‹è¯• ---
if __name__ == "__main__":
    print(" [Audio Module] ç‹¬ç«‹æµ‹è¯•")
    try:
        predictor = EmotionPredictor()
        print("âœ… åˆå§‹åŒ–æˆåŠŸ")

        # äº¤äº’æµ‹è¯•
        while True:
            path = input("\nğŸ”Š è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (è¾“å…¥ q é€€å‡º): ").strip().strip('"')
            if path.lower() == 'q': break

            res = predictor.predict(path)
            print("é¢„æµ‹ç»“æœ:")
            print(res)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
