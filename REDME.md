- # 🌟 EmoChat Pro: 基于多模态融合与大语言模型的情感交互系统

  ![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
  ![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)
  ![Vue3](https://img.shields.io/badge/Vue.js-3.x-4FC08D.svg)
  ![DeepSeek](https://img.shields.io/badge/LLM-DeepSeek-blueviolet.svg)

  ## 📖 项目简介
  在技术高速发展的今天，人们的心理健康问题日益严重。本项目致力于构建一个能更“贴心”、感知力更强的心理辅导帮手，打破单一模态在情感识别中的局限性（如“笑着说反话”或“面无表情的愤怒”）。

  本项目构建了一个能够同时处理**视觉（Vision）、听觉（Audio）、文本（Text）**信息的融合感知系统，并接入 DeepSeek 大语言模型，实现了具有高度“共情能力”的动态对话反馈机制。

  ## ✨ 核心亮点

  * 🧠 **全自主构建的多模态感知网络**：自研音频特征判断网络，并结合文本与视觉大模型提取深层情感特征。
  * 💬 **“感知-理解-共情”一体化对话**：创新的“感知世界观”提示工程，指导 LLM 处理多模态冲突，并支持动态人格（心理疏导、模拟面试、情绪辩论等）与自定义智能体。
  * ⚡ **沉浸式非阻塞交互体验**：前端采用三栏式沉浸布局与实时图表渲染，后端基于 FastAPI 实现高并发双链路并行，保证实时视觉流与重负载分析互不干扰。

  ## 🏗️ 系统架构与核心模态

  ### 1. 文本模态 (Text) —— 核心语义提取
  * **模型选型**：基于 `RoBERTa-Large` 进行 7 分类情感微调（包含 3.55 亿参数）。
  * **数据集**：对 Google 的 `GoEmotions` 数据集进行清洗与标签重映射，构建了 18,098 条高质量中/英情感数据集。
  * **性能表现**：经过多轮超参数调优（Stage 2: Optimized V1），模型在测试集上达到了 **88.06% 的准确率** 与 **0.8789 的 F1-Score**，且单步推理响应保持在 100ms 以内。

  ### 2. 听觉模态 (Audio) —— 时序情感建模
  * **模型选型**：全自主设计的 `ACRNN_v3` (Attention-based CRNN) 架构。
  * **网络结构**：包含动态残差骨干网 (Dynamic ResBlock + CBAM)、Bi-LSTM 时序建模与 Transformer Attention Head，能够自动聚焦语音中的“高光时刻”。
  * **训练与性能**：融合并清洗了 8 大开源数据集（共 4.3 万样本），引入 Mixup 数据增强与自适应学习率调度，最终模型准确率达到 **77.65%**。

  ### 3. 视觉模态 (Vision) —— 显性情绪捕捉
  * **模型选型**：选用 `DeepFace` 作为视觉模态基座。
  * **优势**：相比传统的几何特征方法和轻量级 CNN，该方案在复杂光照和真实环境测试中表现出极佳的工程成熟度与结果稳定性，有效避免了过度偏置。

  ### 4. 决策融合机制 (Multimodal Fusion)
  * **优化策略**：在 MELD 验证集上实施了全参数空间的网格搜索（约 1300 种权重组合）。
  * **最优权重**：获取了 $w_{audio}=0.28, w_{text}=0.66, w_{vision}=0.06$ 的最优分配（F1=0.4678），超越了所有单模态模型的独立表现。
  * **进阶探索**：项目团队同时深入探索了混合多层融合（引入 MulT 跨模态注意力与 MISA 门控残差融合），在 M3ED 纯中文多模态数据集上完成了前沿验证。
  * 注：本项目在过程中尝试了全参数搜索决策融合与混合多层融合，但由于数据集的特殊性，最终实际应用是采用经验性的确定权重进行决策融合。

  ## 💻 技术栈

  * **前端**：Vue 3 + Vite, ECharts (数据可视化)
  * **后端**：Python 3.10, FastAPI, Uvicorn (非阻塞异步架构)
  * **深度学习**：PyTorch, Transformers, DeepFace
  * **LLM 交互**：DeepSeek API, Prompt Engineering
  * **辅助工具**：Whisper (STT), Deep_Translator

  ## 👨‍💻 研发团队

  本项目由以下团队成员共同完成：

  * **高致远** (项目负责人)：音频模型（ACRNN v2/v3）设计与训练、混合多层融合模型开发优化、全流程统筹。
  * **周梓昱**：项目总体框架构建、文本大模型（RoBERTa）微调优化、数据清洗映射、第一版前后端与大模型 API 搭建。
  * **刘可一**：视觉模型对比选型、初代音频模型搭建、系统优化辅助。
  * **蒋成康**：LLM 提示词工程设计与服务封装、多模态决策融合模块开发及优化辅助。
  * **郭鑫**：现代前端框架迭代、Vue 3 + FastAPI 全栈重构、数据对齐与产品交互体验优化。

  ## 🔮 展望
  项目展现了从底层模型训练到上层全栈应用开发的完整能力。未来期待结合时序上下文建模与 LLM 协同决策等 SOTA 技术，并将本系统应用于具身智能等更广阔的领域。