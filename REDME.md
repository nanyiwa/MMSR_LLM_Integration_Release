# 《基于多模态融合与大语言模型的情感交互系统实验报告》



## 0. 项目分工

- 高致远（负责人）：负责二代三代audio模型和fusion混合融合部分模型数据集的筛选清洗，模型的搭建、训练，优化分析，深度参与项目全过程，PPT制作，撰写实验报告

- 周梓昱：整个项目框架的总体构建；文本部分模型的选择，BERT预训练模型的微调、对应数据集的搜集、选择与清洗；多模态fusion融合、前后端以及接入Deepseek的API的第一个版本搭建，撰写实验报告3.2文本模态部分

- 刘可一：视觉模态模型的比较选择，搭建训练一代audio模型，辅助二代三代audio模型和fusion混合融合部分的训练和优化分析，制作项目介绍视频

- 蒋成康： 搭建LLM 对话服务与提示工程部分，完成fusion决策融合部分，辅助二代三代audio模型和fusion混合融合部分的训练和优化分析，PPT制作，撰写实验报告3.1视觉模态部分

- 郭鑫：负责新的前端框架的选择，以及第二版和第三版前端、后端的重构，接收对齐其他组员最新成果，并更新项目成果文件，制作项目介绍视频

团队成员从开始选题讨论，到动手实践，最后汇报与收尾工作都精诚团结，互相帮助，共同完成本次项目。

---

==注==：以下是完整的项目介绍文档，相比与答辩时的增量内容主要是3.2文本模型的重新微调，以及细节优化与实验报告撰写。

---



## 1. 项目背景

- **1.1 问题陈述**：
  - 关于技术高速发展的今天，人们的心理健康问题却日渐严重，我们团队有3名成员长期在学校心理中心志愿工作，对此问题认识更加深刻，故想构建一个能更“贴心”、感知力更强的心理辅导帮手。
  - 单一模态（仅靠文本或仅靠面部表情）在情感识别中有严重的局限性，例如：笑着说反话、面无表情的愤怒不能被准确识别。
- **1.2 项目目标**：构建一个能够同时处理**视觉（Vision）、听觉（Audio）、文本（Text）**信息的**融合感知**系统，并接入DeepSeek大**语言模型**实现具有“共情能力”的对话反馈。
- **1.3 创新点预览**：
  - 全自主构建的面向情感识别的音频特征判断网络。
  - 多模态决策融合，提升复杂场景下情感识别的鲁棒性。
  - 构建“情感感知-语义理解-共情生成”一体化对话机制，基于大语言模型实现情感适配的个性化反馈生成。




---



## 2. 系统架构

- **感知**：多模态感知
- **融合**：决策融合
- **服务**：大模型对话服务

![系统架构图](C:\Users\lenovo\Desktop\ai导论项目收尾工作\系统架构图.png)



---



## 3. 核心模态模型实现

### **3.1 视觉模态（Vision）**

视觉模态在本项目中用于捕捉用户的非语言情绪线索，并作为多模态融合决策的重要组成部分参与最终情感判断。与文本和语音不同，视觉信息更直接反映情绪的外显状态，因此其核心要求并非在理想数据集上的极限性能，而是在真实复杂场景下输出**稳定、可信的情感判断结果**。

基于这一目标，我们对多种主流视觉情感识别方案进行了调研与对比。

#### 方案 A：基于几何特征的传统方法（Dlib + Facial Landmarks）

该类方法通过人脸关键点检测，提取面部几何结构特征（如眉毛角度、嘴角开合度等），并结合传统分类器完成情感判断。其优势在于计算成本极低，实现简单，能够在 CPU 环境下实现实时推理。

然而在实际使用中，该方法对光照变化、拍摄角度及局部遮挡极为敏感，且由于特征依赖显式几何结构，难以刻画面部肌肉的细微变化。在面对低强度或复合型情绪（如轻微厌恶、隐性悲伤）时，该类方法稳定性不足，输出结果波动较大，因此不适合作为多模态系统中可靠的视觉情感来源。


#### 方案 B：轻量级卷积神经网络（FER 库 / Mini-Xception）

轻量级卷积模型在推理速度和部署成本方面具有明显优势，适合算力受限的应用场景。Mini-Xception 等模型在简单背景下能够较快给出情感预测结果。

但在真实环境测试中，我们发现该类模型在复杂背景和光照条件下误检率较高，且预测结果中存在明显的“中性（Neutral）”偏置，容易掩盖其他弱情绪类别。在多模态融合框架下，这种偏置会削弱视觉模态对整体决策的区分能力，因此未被采用。

#### 方案 C：自训练或迁移学习的通用深度模型（如 ResNet 系列）

基于通用卷积网络进行迁移学习是学术研究中常见的基线方案，其优势在于模型结构与训练流程高度可控。但该路线对数据规模、标签质量和训练调参过程要求较高，在有限的项目周期内，难以在泛化能力与工程稳定性之间取得理想平衡，整体投入产出比不高，因此未作为最终方案。


#### 最终选择：DeepFace

综合上述对比，本项目最终选择 DeepFace 作为视觉模态的情感分析模型，与音频和文本模型并列参与多模态融合决策。DeepFace 在实际使用中表现出较好的工程成熟度和环境适应性，其输出结果稳定且形式统一，便于与其他模态进行对齐和融合。基于这些因素，DeepFace 能够在整体系统中可靠地承担视觉模态的情感感知角色。

---

### **3.2 文本模态（Text）**：

**最终选择**：本人（周梓昱）选择**用处理过的GoEmotions数据集（标签重映射与清洗）为基础，用 RoBERTa-large 对 CSV 格式的情感文本数据做 7 分类微调。**

**项目构建的思路**：为了与蒋成康、刘可一同学调研并选择的Deepface的输出维度对齐（即**Happy, Sad, Angry, Fear, Surprise, Disgust, Neutral**这七种核心情感），文本模态必须输出相同维度的概率分布，从而为最终的多模态融合提供标准化的数据输入。总的来说，文本部分主要由两个部分构成，一个是数据集的选择与清洗，另一个是模型的选择与微调。

#### 3.2.1**数据集的选择**：

文本情感分析领域有很多经典的数据集，本项目在对比之后最终选择了Google Research 发布的 **GoEmotions** 数据集。下面是选择 GoEmotions 的原因。

需要说明的是，当前的主流文本情感分析领域的数据集都是英文的，所以我们也自然选择了英文数据集。针对输入文本是中文的情况，我们使用了deep_translator 库中的 GoogleTranslator，它通过调用 Google Translate 的后端 API，实现了全自动的文本翻译，也自然将处理中文文本的问题转化成了处理英文文本的问题。然后考虑到将用户的录⾳⽂件转录为⽂字的需求，我选择 Whisper 这个⽬前开源界最强的语⾳识别模型之⼀，它⽀持多语⾔且鲁棒性极⾼。我们可以对 Whisper 转录出来的⽂本进⾏语义分析，判断⽂字内容所表达的情绪。

**（1）现有主要的数据集的分析**

- **SST-5 (Stanford Sentiment Treebank)**：主要关注情感极性（正向/负向）的强弱，而非具体的离散情感分类，更不用说本项目想要的七种分类了。

- **IMDb/Amazon Reviews**：虽然数据量庞大，但仅限于二元情感分类（褒/贬），无法区分 Fear 与 Sad 等七种细腻的维度。

- **ISEAR**：虽然包含具体情感，但数据量较小且年代久远，语言风格与现代社交媒体口语存在偏差。
- **DAIR-AI Emotion**：来自Twitter的消息文本，但只有六种基本情绪，多了 love 这个情绪，但缺失了 Neutral 和Disgust 这两类情绪。

**（2）GoEmotions的优势**：

- **细粒度标注**：原生包含 28 种精细情感标注，涵盖了人类情感的绝大部分情况。
- **规模与真实性**：源自 Reddit 评论，包含 5.8 万条真实语境下的对话，数据量大，极其符合本系统面向实时社交交互的应用场景。
- **可扩展性**：其丰富的标签体系允许我们通过“标签蒸馏”较为精准地**映射到目标 7 维度**，这个是我选择它的关键。

**（3）数据工程的具体流程**：

主要是进行标签的映射，我将相似情感进行了逻辑聚合，得到的表格如下。经过清洗与映射，我剔除了多标签冲突的模糊样本，最终构建了一个包含 **18,098 条数据** 的精简高效数据集（`my_emotion_dataset.csv`），确保了分类边界的清晰性。

| **目标映射的情绪类型** |      **GoEmotions 对应的情绪种类 (Origin)**       |   **映射逻辑**   |
| :--------------------: | :-----------------------------------------------: | :--------------: |
|       **Happy**        | joy, love, admiration, optimism, relief, pride 等 |   正向情感聚合   |
|       **Angry**        |         anger, annoyance, disapproval 等          |  冲突性情绪聚合  |
|        **Sad**         |    sadness, disappointment, grief, remorse 等     | 负向低唤醒度聚合 |
|        **Fear**        |               fear, nervousness 等                |  避害性情绪聚合  |
|      **Surprise**      |  surprise, realization, confusion, curiosity 等   |  突发性认知聚合  |
|      **Disgust**       |                      disgust                      | 强排斥性情绪保持 |
|      **Neutral**       |                      neutral                      |   基准状态保持   |

#### 3.2.2**微调模型的选择**：

12月28号答辩前，我选择了 **DistilBERT**（BERT 的轻量化蒸馏版本，但是保留了 BERT 97% 的性能）；期末考试之后，我选择尝试用更 SOTA 的 **RoBERTa-Large** 去替代 DistilBERT ，从而实现对文本模型部分的升级，也是我们项目**相比答辩时候的一个增量**。

下面是对这两个模型的一个简单比较：

- **DistilBERT**：DistilBERT 是 BERT 的精简版本，通过知识蒸馏技术保留了约 97% 的语义理解能力，但参数量仅为 66M。因此它训练时间短，且准确率不低，为了在答辩前快速出成果，我选择在答辩前使用它；同时它的推理速度极快，适合作为项目的初步可行性验证（Baseline）。

- **RoBERTa-Large** ：RoBERTa（Robustly Optimized BERT Approach）是 Meta 对 BERT 的改进版。相比于 DistilBERT，RoBERTa-Large 拥有 3.55 亿的参数量（24 层 Transformer 深度）。

从核心原理上来看，RoBERTa 移除了 BERT 的下一句预测（NSP）任务，改用动态掩码机制，并在更大的语料库上进行了更长时间的训练。这使其在捕捉文本中微妙的情感转折（如讽刺、潜在的恐惧）时，比 DistilBERT 具有更强的表征能力。

#### **3.2.3 本地硬件选择**

本人使用的是Mac Air笔记本。为了追求泛化性能，本人使用 Apple Silicon 的统一内存架构（Unified Memory Architecture）与 MPS 加速（ M3 芯片），针对这两款模型进行了多轮次的微调实验。

#### 3.2.4 实验全记录与性能对比

本实验在本地预处理的 7 分类情感数据集上进行了多轮迭代，来寻找模型复杂度、收敛速度与泛化能力之间的最佳平衡点。实验详细数据记录如下表。

| **实验阶段**               | **模型架构**      | **关键超参数 (LR / Decay)** | **准确率 (Acc)** | **F1-Score** | **训练耗时 (M3 GPU)** | **实验结论**                                          |
| -------------------------- | ----------------- | --------------------------- | ---------------- | ------------ | --------------------- | ----------------------------------------------------- |
| **Stage 1 (Baseline)**     | DistilBERT        | 2e-5 / 0.01                 | 87.51%           | 0.8710       | 30 分钟               | 性能稳健，效果也非常好，很适合作为项目基础指标        |
| **Stage 2 (Optimized V1)** | **RoBERTa-Large** | 1e-5 / 0.01                 | **88.06%**       | **0.8789**   | 3 小时                | **综合性能最强，确定为最终版本**                      |
| **Stage 3 (Optimized V2)** | RoBERTa-Large     | 7e-6 / 0.05                 | 84.74%           | 0.8438       | 5.5 小时              | 过度正则化导致性能倒退                                |
| **对比组**                 | Siebert-SOTA      | 直接推理 (Zero-shot)        | 32.17%           | 0.2594       | N/A                   | 维度不适配，无法直接应用于 7 分类，证明本地微调必要性 |

注：我在实验中引入了 Hugging Face 社区公认的 **SOTA 模型 `siebert/roberta-large-english-emotion`** 针对GoEmotions数据集作为对比，虽然该模型在通用情感任务上表现卓越，但由于该模型原生仅支持 6 类情感，缺失了本项目核心的 **Neutral（中性）** 和 **Disgust（厌恶）** 维度，所以在面对 7 维度任务时，其准确率仅为 **32.17%**。这也充分说明了针对特定任务进行**数据清洗与模型微调**的必要性。

下面是我在三个阶段（3 Stages）的**超参数**的选择的汇总表。

| **超参数项目**              | **Stage 1 (Baseline)**    | **Stage 2 (Optimized V1)** | **Stage 3 (Optimized V2)** |
| --------------------------- | ------------------------- | -------------------------- | -------------------------- |
| **模型架构**                | DistilBERT                | RoBERTa-Large              | RoBERTa-Large              |
| **预训练权重 ID**           | `distilbert-base-uncased` | `roberta-large`            | `roberta-large`            |
| **参数量 (Parameters)**     | 66M                       | 355M                       | 355M                       |
| **学习率 (Learning Rate)**  | 2e-5                      | 1e-5                       | 7e-6                       |
| **训练轮数 (Epochs)**       | 3                         | 3                          | 5                          |
| **等效 Batch Size**         | 16                        | 16                         | 16                         |
| **物理 Batch Size**         | 16                        | 4                          | 4                          |
| **梯度累积步数**            | 1                         | 4                          | 4                          |
| **权重衰减 (Weight Decay)** | 0.01                      | 0.01                       | 0.05                       |
| **预热比例 (Warmup Ratio)** | 0.1                       | 0.1                        | 0.15                       |
| **学习率调度器类型**        | Linear (线性)             | Linear (线性)              | Cosine (余弦退火)          |
| **最大序列长度**            | 128                       | 128                        | 128                        |
| **随机种子 (Seed)**         | 42                        | 42                         | 42                         |
| **硬件加速设备**            | Mac M3 (MPS)              | Mac M3 (MPS)               | Mac M3 (MPS)               |

#### 3.2.5 训练结果的深度分析与最终选择

（1）本人认为 Optimized V1 (Stage 2，使用RoBERTa-Large模型架构) 是最佳选择。

我发现 Stage 2 虽然在训练时长上比DistilBERT的基础组增加了 6 倍，但有着下面两个明显的进步。

首先是**核心类别的召回率提升**。RoBERTa-Large 在识别难度较大的“悲伤 (Sad)”与“恐惧 (Fear)”维度上，召回率比 DistilBERT 提升了约 **11.9%**。这证明了大模型在处理复杂语义边界时具备更强的敏感度。

其次是**精准的分类平衡**。Optimized V1 的 Weighted F1-score 达到了 0.8789，这说明模型不仅在样本量大的“中性”类上表现好，对“厌恶”等小样本类也保持了极高的精确度。

同时也有**效率的保证**。在推理阶段，RoBERTa-Large 的单步响应保持在 **100ms** 以内，确保了三模态系统在前端演示时的实时性。

（2）关于 Stage 3 (V2) “失败”实验的反思

在 Stage 3 中，我尝试引入了更强的正则化手段（Weight Decay 从 0.01 提升至 0.05）以及更低的学习率和余弦退火策略。虽然训练过程长达 5.5 小时且收敛曲线平滑，但最终准确率反而下降了约 3.3%。这反映了在 1.8 万条规模的数据集下，RoBERTa-Large 存在**“过度抑制”**风险。过强的权重衰减导致模型无法充分拟合 GoEmotions 数据集中的微弱情感特征，过强的正则化（Regularization）约束了模型在特定领域的表现，导致其未能达到局部最优解。Stage 3 的实验提供了宝贵的参数边界参考，说明了 Stage 2 的超参数组合（1e-5 / 0.01）正是本任务的“甜点区”。

（3）最终模型选择：**Stage 2 (Optimized V1，使用RoBERTa-Large模型架构)**

最终本人选择封装 **Stage 2 (Optimized V1，使用RoBERTa-Large模型架构)** 的权重作为文本模态的核心。它是本人认为在针对该项目在学术研究深度（模型规模升级）与工程实用性（高准确率）之间的较优平衡。

### **3.3 听觉模态（Audio）**：

- **数据集获取清洗**：

  - 最终版筛选清洗融合8项数据集（CREMA-D，EmoDB，ESD，JL_Corpus，MELD，RAVDESS，SAVEE，TESS），总样本量达4.3万。
  - 清洗数据集：包括视频转音频，音频转Log-Mel 谱图，标签对齐等等操作。
  - 下图显示最终融合数据集的标签分布
  - ![最终融合数据集的标签分布](C:\Users\lenovo\Desktop\audio部分训练记录\v3阶段\最终融合数据集的标签分布.png)
  - 下图显示各数据集的贡献
  - ![v3_source_emotion_stack](C:\Users\lenovo\Desktop\AI_Intro_project\v3_source_emotion_stack.png)

- **模型架构**：这部分详细介绍 **ACRNN_v3**（Attention-based CRNN）的设计。

  - 下图是audio最终模型（ACRNN_v3）流程图

    ![audio模型流程图-横向](C:\Users\lenovo\Desktop\AI_Intro_project\audio模型流程图-横向.png)

  - 1、输入与预处理层 (Input Stem)

    - **输入数据**：Log-Mel Spectrogram (对数梅尔谱图)。
      - 维度：$(B, 1, 128, 130)$ —— (Batch, Channel, Freq, Time)。
    - **Stem Layer**：
      - 操作：`Conv2d (3x3)` $\to$ `BatchNorm` $\to$ `ReLU` $\to$ `MaxPool`。
      - 作用：快速降低特征图分辨率，提取初步的纹理特征。

  - 2、动态残差骨干网 (CNN Backbone: Dynamic ResBlock)

    这是特征提取的核心，采用了金字塔结构。

    - **结构**：由 4 个级联的 **Dynamic ResBlock** 组成。
    - **通道变化**：Filter 数量逐层加宽：`[64] -> [128] -> [256] -> [512]`。
    - **内部组件 (ResBlock)**：
      - `Conv2d` + `BN` + `ReLU`
      - **CBAM 模块** (关键点)：包含通道注意力 (Channel Attn) 和空间注意力 (Spatial Attn)。
      - **Residual Shortcut**：残差连接 $(x + F(x))$，防止梯度消失。
      - **MaxPool**：在每个 Block 后进行下采样

  - 3、维度重塑与投影 (Reshape & Projection)

    连接 CNN 和 RNN 的桥梁。

    - **维度变换**：将 CNN 的输出 $(B, C, F, T)$ 转换为 RNN 需要的时序格式 $(B, T, Feature)$。
      - 代码逻辑：`permute(0, 3, 1, 2)` $\to$ `reshape`。将频率维度(F)和通道维度(C)展平，保留时间维度(T)。
    - **线性投影**：通过 `Linear` 层将展平后的特征映射到 RNN 的输入维度。

  - 4、时序建模与注意力 (Temporal Modeling & Attention)

    - **Bi-LSTM**：
      - 双向 LSTM，层数=2。
      - 作用：捕捉语音中的长距离上下文依赖（例如语调的抑扬顿挫）。
    - **Transformer Attention Head**：
      - 使用 `MultiheadAttention` 。
      - 作用：不只是简单的取平均，而是计算不同时间步之间的关联，自动聚焦于情感最强烈的“高光时刻”。

  - 5、全局聚合与分类 (Aggregation & Classifier)

    - **Global Mean Pooling**：在时间维度上取平均，得到句子级的特征向量。
    - **MLP Head**：
      - `Linear` $\to$ `ReLU` $\to$ `Dropout` $\to$ `Linear`。
      - Logits $\to$ 概率。
      - 输出：7 分类概率。

- **模型优化过程**：

  - 简单模型v1

    - 简单架构（基础CNN+RNN+Attention模式），小规模训练（1440的平均7分类样本），迅速实现，为后端提供调用接口。

  - 加强模型v2

    - v2_0_baseline（测试集准确率62%）：结构复杂化，添加新的处理模块，例如

      - Mixup 数据增强：
        - 在训练过程中，以 $\alpha=0.2$ 的超参数对随机两个样本及其标签进行线性插值混合（$\tilde{x} = \lambda x_i + (1-\lambda) x_j$）。
        - 作用：迫使模型在特征空间中学习线性的类间过渡，极大地增强了模型在面对模糊情感（如“悲伤”与“中性”之间）时的泛化能力，并有效防止了模型对特定噪声模式的死记硬背。
        - “异常”分析：训练集准确率 (Train Acc) 一直低于验证集准确率 (Val Acc)，这说明Mixup 数据增强起到了作用：训练时，模型看的是“两张脸叠在一起”的变态数据（Hard Mode），很难猜对；验证时，模型看的是干净数据（Easy Mode），所以表现更好。所以这其实并非”异常“。
      - 自适应学习率调度：
        - 动态监控验证集 Loss，当指标不再下降时自动衰减学习率（Factor=0.5），帮助模型在损失函数的局部最小值附近进行更精细的收敛。

    - 大规模数据集训练2.5w（1.3w纯净的训练音频+1.2w含环境噪声如笑声、掌声、音乐的训练音频）

    - 独立结构调整优化

      - v2_1_clean：去除1.2w含背景噪声的数据集，只用纯净的训练音频训练，验证模型的有效性（至少在低噪环境下有效），达成测试集准确率79%。

      - v2_2_balanced：训练时给总量小的标签的样本更高权重，原始数据集中netural标签的数据集占比最大，导致训练出的模型在不确定时非常偏向于直接选择netural标签，其他标签的样本召回率远低于netural。

        - base_line结果**62.08%** VS balanced结果**60.02%**：这是一个看似矛盾的点，按道理balanced后肯定效果更好，但是总准确率却下降了，但比较混淆矩阵分析后，可以发现60.02%其实是更成功的。这里出现了**准确率悖论**，base_line由于netural标签占比大，训练后的模型偏向于在测试集中更多地选择netural，balanced处理后，模型没那么偏向于直接选择netural，对其他标签的召回率显著提高，但对netural的识别能力减弱（其实不能说是能力减弱，本来更像是不思考直接蒙netural，现在有了思考），由于netural数量占比大，故总准确率反而下降，但实际性能其实是提升。（这里可见总准确率其实不能完美反应模型性能，还需要分析召回率、F1分数等其他指标）

        - | **情绪类别** | **Baseline 召回率 ** | **Balanced 召回率 ** | **变化** |
          | ------------ | -------------------- | -------------------- | -------- |
          | **Surprise** | 0.35                 | **0.52**             | **+17%** |
          | **Happy**    | 0.37                 | **0.46**             | **+9%**  |
          | **Fear**     | 0.55                 | **0.64**             | **+9%**  |
          | **Sad**      | 0.54                 | **0.62**             | **+8%**  |

      - v2_3_wider：增加模型复杂度，模型“加宽” 提升脑容量（模型复杂度不足会导致训练欠拟合）。准确率涨到了**62.68%**，带来了性能提升，但提升幅度较小。观察日志这次100 轮训练跑满，没有触发早停，这说明大模型的潜力确实更大，它还在慢慢学，直到最后Loss还在微弱震荡，说明我们也需要更多的数据集来训练它。

      - v2_4_smooth：标签平滑的参数调整（强迫模型不敢轻易把概率全压在 Neutral 上）。准确率涨到了**62.28%**，这是一种安全且有效的优化手段。虽然整体准确率仅微涨 0.2%，但它成功将对Happy的识别率提升了6%，温和地缓解了Neutral类别的黑洞。

      - 以上几次优化为独立修改测试，在v2_0_baseline上修改，确保分析清晰有效，每次均独立通过训练日志、混淆矩阵、loss下降曲线等方法即时做详细分析。

  - 最终模型v3

    - 以v2的架构为基础，融合balanced、wider、smooth思想。

      - 直接融合后模型训练发生崩溃，后在超参数调整部分详细分析。

    - 超参数调整，进行了`4*2*2=16`次超参数调整测试，每次跑24轮（而不是按原先100轮训练，为了节约时间和算力）

      - learning_rate：0.0001，0.0003，0.0005，0.0008

      - batch_size：16、32

      - label_smoothing：0.1、0.2

      - | **实验 ID** | **Learning Rate** | **Batch Size** | **Label Smoothing** | **Best Val Acc** |
        | ----------- | ----------------- | -------------- | ------------------- | ---------------- |
        | **Exp 1**   | **0.0001**        | **16**         | **0.1**             | **76.42%**       |
        | **Exp 2**   | 0.0001            | 16             | 0.2                 | 76.05%           |
        | **Exp 3**   | **0.0001**        | **32**         | **0.1**             | **76.67%**       |
        | Exp 4       | 0.0001            | 32             | 0.2                 | 75.60%           |
        | Exp 5       | 0.0003            | 16             | 0.1                 | 74.85%           |
        | Exp 6       | 0.0003            | 16             | 0.2                 | 74.96%           |
        | Exp 7       | 0.0003            | 32             | 0.1                 | 75.32%           |
        | Exp 8       | 0.0003            | 32             | 0.2                 | 75.89%           |
        | Exp 9       | 0.0005            | 16             | 0.1                 | 25.63%           |
        | Exp 10      | 0.0005            | 16             | 0.2                 | 25.63%           |
        | **Exp 11**  | 0.0005            | 32             | 0.1                 | **73.87%**       |
        | **Exp 12**  | 0.0005            | 32             | 0.2                 | **25.63%**       |
        | Exp 13      | 0.0008            | 16             | 0.1                 | 25.63%           |
        | Exp 14      | 0.0008            | 16             | 0.2                 | 25.63%           |
        | Exp 15      | 0.0008            | 32             | 0.1                 | 25.63%           |
        | Exp 16      | 0.0008            | 32             | 0.2                 | 25.63%           |

      - 最终选定 {'learning_rate': 0.0001, 'batch_size': 32, 'label_smoothing': 0.1}的超参数组合。（超参数调整测试前的v3_0版本选择的超参数组合是 {'learning_rate': 0.0005, 'batch_size': 32, 'label_smoothing': 0.2}）

      - 超参数调整中出现的“反常”现象与分析：

        - v3_0版本的超参数选择和超参数调整测试的第十二个实验完全相同都是{'learning_rate': 0.0005, 'batch_size': 32, 'label_smoothing': 0.2}，但v3_0版本训练出的模型效果相当好，最终测试集准确率77.77%，而在第十二个实验时模型训练发生了崩溃。

        - 由实验结果表格可见，超过0.0003的学习率的超参数组合都极容易发生崩溃，对比实验11和12，LR=0.0005, BS=32相同，仅Smooth改变，而结果完全不同，可见LR=0.0005可以几乎看为一个临界崩塌的学习率取值，其他条件稍微变化一点，结果可能就完全不同。

      - v3_0版本的训练可以认为是一次幸运的激进尝试，由于模型在初始化时权重随机生成，在 `LR=0.0005` 这个激进的参数下，某一次随机初始化可能恰好处于“好跑”的位置（凸性较好的区域），模型就收敛了。而超参数测试中的实验12，可能随机到了一个“难跑”的初始位置，配合高学习率，直接导致模型崩塌。

- **最终成果**：

  - 选定 v3_1{'learning_rate': 0.0001, 'batch_size': 32, 'label_smoothing': 0.1}的**77.65%**（跑完100轮的最优结果）作为最终部署模型，而不是v3_0{'learning_rate': 0.0005, 'batch_size': 32, 'label_smoothing': 0.2}的**77.77%**。



---



## 4. 多模态融合与决策机制

多模态融合是指将文本、语音、视觉等不同模态的信息整合，以提升任务性能，其核心是解决 “模态异构性”（不同模态数据格式、语义表达差异大）和 “信息互补性”（各模态提供独特信息）的问题。多模态融合常见分类为**早期（特征）融合**，**后期（决策）融合**以及**混合（多层）融合**，我们的项目中先后尝试了后期决策融合和混合多层融合。

- **后期决策融合**

  后期决策融合是各模态独立建模，聚合决策结果，优点是可以复用单模态的成果，简单直接，且能降低单模态噪声的影响，缺点是不能利用模态间的深层语义关联，性能通常低于早期融合和混合融合。

  - 在**项目之初**，为了快速搭建起整个项目的基本框架，我们使用了简单加权的决策融合方法，为三模态平均分配权重，简单相加作为多模态的决策结果。
  - 在尝试混合多层融合、但结果不理想**之后**，我们回到后期决策融合方法，并进行了基于**全参数网格搜索的权重优化**。为了确定最优的模态融合权重，我们摒弃了经验主义的手动调参，而在**MELD验证集**上实施了全参数空间的网格搜索，我们定义步长为0.02，遍历了满足w_a + w_t + w_v = 1且w_i>=0的所有权重组合（约 1300 种组合），并根据结果绘制出了权重三元热力图和混淆矩阵，在MELD验证集上最优权重组合是w_a=0.28，w_t=0.66，w_v=0.06，最优权重组合的F1=0.4678。
  - 实验结果显示最优权重组合是由文本内容主导的，这一结果与**ACL 2022 M3ED 原论文**的结论相似，即在对话场景中，文本模态包含最丰富的情感线索；并且结果显示在多模态决策融合机制下，准确率超越了所有单模态的决策（最优权重组合没有滑入某种模态权重为0的情况），体现多模态的优越性。
  - 实验过程中我们不是在每种权重组合下全部将MELD测试集全部重新前向传播三种模态的模型得到结果，而是一次得到测试集的每个样本的三模态结果暂存，然后进行高精度的全参数空间网格搜索，得到最佳权重，这样**极大提高了训练效率**。
  - 我们**选择**在MELD数据集的**测试集**进行权重优化搜索，而不是整个MELD数据集，并不是为了减少程序运行时间，而是考虑到vision、text和audio三模态模型在训练时很可能已经将MELD作为数据集进行训练（vison和text不完全由我们进行训练所以不确定，audio模型在训练时确实使用了MELD数据集）（MELD数据集官方已经划定了训练集、验证集和测试集），这样可以防止使用MELD数据集训练的单模态模型在这里权重优化实验中表现更出色，而保证实验的**严谨性**。

<img src="C:\Users\lenovo\Desktop\MELD数据集下的混合决策\固定参数\三元热力图.png" alt="三元热力图" style="zoom: 33%;" />

<img src="C:\Users\lenovo\Desktop\MELD数据集下的混合决策\固定参数\混淆矩阵_固定参数.png" alt="混淆矩阵_固定参数" style="zoom:33%;" />

- **混合（多层）融合**

  在神经网络的**中间层**进行交互。三模态的特征在进入分类器之前，先经过专门设计的**交互层**（例如 Attention, Gating, Bi-LSTM），让模态之间互相“交流”，优点是能捕捉深层次的跨模态语义，缺点是网络结构复杂，设计难度大。

  - 在正式进入网络构建前，我们团队学习了多模态相关论文，阅读了多篇多模态领域经典论文以及最前沿的情感多模态识别的相关论文，例如DAG-ERC(2021)、Robust Multimodal Emotion Recognition（2021）、MultiEMO（2023）、InstructERC（2024）、Towards robust multimodal emotion recognition in conversation with multi-modal transformer and variational distillation fusion（2025.9）等等。

  - **数据集的选择**：我们有3次**转变**：

    - **IEMOCAP**：这是学术界**经典**的（2008发布）情感识别、多模态情感识别核心数数据集之一，获取需要向美国南加州大学 SAIL 实验室填写表单发送邮件审核（但是审核得还挺快，不到一天就得到了回复）（我们在训练audio模型的时候就下载好了这个数据集而且特意没有使用，就是为了为现在的多模态融合的时候使用，如果训audio的时候就用了这个数据集可能导致训练结果不公正地偏向于audio模型）。但是经过数据清洗后（数据集是10分类，我们的项目是7分类）发现，数据集的类别非常不均匀，清洗出的5649条有效数据（具体数量在提交文件夹下的过程日志文件夹下的.txt文件）中，netural标签是surprise的17倍，是fear的43倍，因此我们舍弃了这个显然不适合的数据集。
    - **CMU-MOSEI**：这是卡内基梅隆大学（CMU）主导构建的大规模多模态情感分析与情感识别基准数据集（2018发布），目前学术界**最核心**、应用最广泛的标杆数据集之一，但由于其分类标签只有6，加上了解到其数据量极大（原始文件400GB左右），故没有尝试使用。
    - **MELD**：这是南加州大学、卡内基梅隆大学等团队（2018）发布的多模态多参与方情感对话基准数据集，全部来自美剧**老友记切片**。（对我们的项目来说）优势在于三模态数据对齐清晰，7分类标签和我们的项目完美吻合，来自电视剧故对话自然；局限在于由于源自电视剧切片，视觉和音频模态的噪声极大，比如视觉镜头经常切换、背景复杂、人脸有时很小、有时是侧脸、有时画面里有好几个人等等原因，训练出的结果不理想，故再次尝试其他数据集。（不仅我们的训练结果不理想，目前学术界**SOTA水平**在MELD数据集上的F1（65%左右）也较低，比如上面提到的几篇论文）
    - **M3ED**：这是中国人民大学、香港中文大学（深圳）、新加坡国立大学联合团队，2022 年首次发表于 ACL 会议首个大规模**中文**多模态情感对话数据集，来源于56 部中文电视剧的 990 组**双人情感对话**，优势在规模较大（我们提取出24,449 条样本），标签标注好（支持多标签的概率，而不是一个标签概率的100%），对我们的项目来说劣势在于由于电视剧的版权问题，它不直接提供原始音视频文件，而是直接提供处理过的vision和audio的特征向量，这样我们无法测试我们的vision和audio模态的模型；而且标签不平衡问题严重，Neutral (10028) vs Fear (395)：比例高达 **25:1**。

  - **特征提取**：

    - 我们是将三模态的信息分别给vision（VGG-Face）、text（bert）、audio（自训）三模态模型处理为三组特征向量（2622D、768D、256D）保存入`diaX_uttX_audio.npy` (音频特征)、`diaX_uttX_vision.npy` (视觉特征)、`diaX_uttX_text.npy` (文本特征)。
    - 其中视频抽帧策略是均匀地从视频中抽取五帧，分别通过VGG-Face模型提取特征，剔除全0向量（未检测到人脸会输出全0向量）后将特征向量取平均，而不是鲁莽地随意抽帧 或者 对图片“取平均”变成糊脸后提取特征。

  - **构建网络**

    - **输入 X (Input)**：每个样本是三个特征向量A、T、V：

      听觉向量：长度 256 (来自 ACRNN 的 Bi-LSTM 输出)

      文本向量：长度 768 (来自 BERT 的 CLS Token)

      视觉向量：长度 2622 (来自 DeepFace 的 VGG-Face)

    - **模态投影与对齐** (Unimodal Projection)

      - 因为 Audio (128维)，Vision (2622维)， Text (768维) 长度不同，我们首先用全连接层（Linear Layer）把它们都映射到同一个维度（例如 D=256）。

    - **跨模态注意力** (Cross-Modal Attention) 

      - 借鉴论文：**MulT (Multimodal Transformer, ACL 2019)**
      - 我们不是使用简单的 A+V+T，而是以文本为主干设计了两个注意力模块Text queries Audio和Text queries Vision，动态提取音视频中的互补信息。这样的操作可以动态捕捉模态间的对齐关系，比如文本是反讽（字面是夸奖），音频是嘲讽语气，Attention 能捕捉到这种“不一致性”。

    - 门控残差融合 (Gated Residual Fusion) 

      - 借鉴论文：**MISA (ACM MM 2020)** 
      - 注意力机制虽然好，但有时候音频全是噪音，或者视频里没拍到脸（全黑）。为了防止坏数据污染好的文本特征，我们引入了门控（Gating）和残差连接（Residual）。
      - 具体操作：
        1. **门控**：$Gate = \sigma(W \cdot Feature)$。计算一个 0~1 的系数，如果特征质量差，系数自动趋近 0。
        2. **残差**：我们将处理后的 Audio/Vision 特征，加回到**原始文本特征**上。
        3. **最终融合**：$F_{final} = Text + \alpha \cdot (Attended\_Audio) + \beta \cdot (Attended\_Vision)$。
      - **优势**：保证了**“保底效果”**,即使在最坏的情况下，模型也只会退化为一个纯文本情感分析模型（bert），依然有很高的准确率，不会因为audio或vision模态的噪声而崩盘。

    - 多任务学习头 (Multi-Task Learning Heads) 

      - **借鉴论文**：**Scientific Reports 2025 (MTFN)** & **HACMFN**
      - 网络的尾部叉开成两条路：
        - **Head 1 (Main)**：输出 7 个概率值 (Softmax)。
        - **Head 2 (Aux)**：输出 3 个概率值 (Softmax)。
      - **Loss 计算**：$Loss_{total} = Loss_{7class} + \lambda \cdot Loss_{3class}$ （$\lambda$ 通常取 0.3）。
      - 利用粗粒度标签（3分类）提供的额外监督信号，帮助网络更快收敛，并可以解决少样本类别（Fear/Disgust）难训练的问题。

    - **输出 Y (Label)**：包含两个输出头（Head），分别输出 7分类结果和 3分类结果：主任务情绪分类 ( 0=Neutral等)和辅助任务（0/1/2：正/负/中）。

  - **优化网络**（优化前的网络训练出的结果很差）

    - 优化前出现了严重的过拟合，随着训练轮次增多，Loss下降，准确率却也下降，对应优化工作是加强正则化，WEIGHT_DECAY（权重衰减）从 1e-5 增大到 1e-3，DROPOUT（随机失活）：从 0.3 增大到 0.5。
    - 从**Val F1**低于 **Val Acc**和混淆矩阵的结果可以看出在小类别的学习效果很差，优化工作增加了类别加权，比如fear权重改为了netural的十倍。（后面的训练结果证明矫枉过正了）

  - **训练模型及结果分析（MELD数据集上）**

    - 由于MELD数据集自身噪声极大，我们的两版模型的效果都很不理想，分别为Val F1=0.38，Val Acc=0.40和Val F1=0.36，Val Acc=0.36。（目前学术界SOTA水平F1在67%，这是2025年9月份Towards robust multimodal emotion recognition in conversation with multi-modal transformer and variational distillation fusion中的结果，这是数据集发布7年经过无数次技术结构迭代，又引入了大语言模型的结果，也才67%，体现这个数据集确实很难，我们的水平有限是合理的）

    ![Loss&Acc_v0](C:\Users\lenovo\Desktop\MELD数据集下的混合决策\fussion\Loss&Acc_v0.png)

    <img src="C:\Users\lenovo\Desktop\MELD数据集下的混合决策\fussion\混淆矩阵_v0.png" alt="混淆矩阵_v0" style="zoom:50%;" />

    ![Loss&Acc_v1](C:\Users\lenovo\Desktop\MELD数据集下的混合决策\fussion\Loss&Acc_v1.png)

    <img src="C:\Users\lenovo\Desktop\MELD数据集下的混合决策\fussion\混淆矩阵_v1.png" alt="混淆矩阵_v1" style="zoom:50%;" />

    

  - **训练模型及结果分析（M3ED数据集上）**

    - 在MELD数据集上测试完模型后，我们又在该模型基础上针对M3ED数据集的特点做一些改动，比如重新制定类别权重，改变输入视觉特征向量的维度2622->342等，最后F1达到0.45，看着比较低但和该数据集的论文相比其实并不差太多，原作者提出的复杂模型 MDI（融合了特定情感层），F1 也才勉强到 **50.1%**，可见其实M3ED是一个比较难的数据集，我们的混合融合模型效果已经达到了较好的程度。
    - 但由于使用M3ED数据集，不能调用我们自己的vision和audio模型来提取特征，故F1虽达到了比较接近原论文水平的高度，但是其实并不适合把训练出的模型权重应用到我们的多模态决策模型，故最终我们的产品（交互网页）没有应用该混合模型，而是采取最初的后期决策融合方法。



---



## 5. LLM 对话服务与提示工程

为了赋予系统“人格化”的交流能力并有效利用多模态感知结果，本项目摒弃了简单的 API 调用，而是构建了一套分层的 LLM 服务架构，并设计了包含**感知世界观**与**动态人格**的提示词工程体系。

### 5.1 服务层架构设计 

我们基于DeepSeek大模型构建了对话后端，通过 `src/llm/service.py` 实现了服务封装。与传统的调用大模型一问一答不同，本系统采用了分层解耦的设计：

1. **服务解耦**：将 LLM 的网络请求封装为 `LLMService` 类，通过 `OpenAI` SDK 标准接口连接 DeepSeek API。这种设计屏蔽了底层模型差异，如有需要可无缝切换至其他开源模型。
2. **上下文管理**：系统维护对话历史，每次请求均携带完整的上下文窗口，确保模型能够理解多轮对话中的情感连续性。

### 5.2 提示词工程策略 

为了解决多模态模型准确率不均的问题，并在对话中体现“共情”，我们设计了复杂的提示词构建逻辑：

#### A. 感知世界观注入

这是本项目的创新点之一，我们在系统提示词中注入了**“系统感知世界观”**，明确告知 LLM 如何处理多模态冲突。

- **背景**：在实际测试中，我们发现视觉模型（DeepFace）准确率较高，而音频模型在噪声环境下易误判。

- **策略**：在提示词中硬性规定了**感知优先级**：

  > 1. **视觉模态 (高信度)**：重点参考面部表情。
  > 2. **文本内容 (核心)**：直接阅读用户语义。
  > 3. **音频/文本模型 (低信度)**：仅供参考，降低依赖。

- **目标效果**：例如当音频模型误报“愤怒”但用户面带微笑时，LLM 能够依据此规则，忽略音频噪声，做出符合视觉逻辑的反馈。

#### B. 动态人格表 

为了适配不同的应用场景，我们构建了动态人格表，预设了多种专家模式：

- **日常闲聊**：设定为幽默、接地气的死党，多用 Emoji 和口语化表达。

- **心理疏导**：基于罗杰斯人本主义流派，强调“无条件积极关注”与共情，引导用户宣泄而非评判。

- **模拟面试**：设定为严肃的面试官，具备动态压力调节机制。

- **情绪辩论**：设定为逻辑严密的“杠精”反方，利用情绪状态进行博弈（如指出用户“急了”）。

  每个模式不仅定义了角色，还包含了具体的风格指南，指导 LLM 的语气和用词。

#### C. 全局交互约束 

为了防止 LLM 生成“话题终结者”式的回复，我们引入了全局约束：

- **保持主动**：强制要求模型在回复末尾抛出开放式问题，延续对话流。
- **拒绝冷场**：当用户输入简短时，模型必须主动挖掘深层含义或转移话题。

### 5.3 用户自定义智能体 

为了增加系统的可玩性，系统支持**Prompt 动态注入**。前端 `Sidebar.vue` 允许用户输入自然语言描述（如“你是一个傲娇的猫娘”），后端 `get_system_prompt` 函数会将此描述实时编译进 System Prompt 中，实现了自定义人格设定。

------

## 6. 系统应用开发与交互优化

项目前端采用 **Vue 3 + Vite** 框架，后端基于 **FastAPI** 重构，构建了一个低延迟、高响应的沉浸式多模态情感交互系统。

### 6.1 前端交互架构

系统采用了**三栏式沉浸布局**，各模块各司其职（代码对应 `App.vue`）：

1. **左侧控制舱 (Sidebar)**：
   - 提供模式切换与自定义角色入口。
   - 当用户切换模式时，后端 `/api/greeting` 接口会立即生成符合当前人设的专属开场白（例如切换到面试模式，AI 会立即说：“请坐，我们可以开始了吗？”），增强对话沉浸感。
2. **中控感知台 (PerceptionPanel)**：
   - **全息感知**：通过 `<video>` 与 `<canvas>` 叠加技术，实时渲染摄像头画面。
   - **可视化反馈**：集成了 `ECharts` 图表，将后端返回的 `vision_score`, `audio_score` 等置信度数据实时可视化。这不仅让用户看到了结果，更看到了系统的**“思考过程”**。
   - **交互逻辑**：采用“按住说话-松开发送”的简单交互逻辑，降低用户上手门槛。
3. **右侧交互窗 (ChatWindow)**：
   - **情感显性化设计**：聊天气泡不仅仅显示文字，还附带了**情感标签**和**置信度**。
   - **动态环境色**：系统界面会根据 `currentEmotion` 实时改变主题色（如检测到 ANGRY 时，状态栏光晕变为红色），利用 `box-shadow` 和 CSS 动画创造出呼吸灯效果，从视觉层面强化情感反馈。

### 6.2 后端编排与并发处理

后端 `main.py` 充当了整个系统的**中枢神经**，解决了多模态推理的高并发与阻塞问题：

- **非阻塞式架构**：利用 FastAPI 的 `async/await` 特性，实现了 `live_vision`（实时视觉流）与 `analyze`（深度多模态分析）的双链路并行。
  - **快链路**：`/api/live_vision` 以 30FPS 的频率处理纯视觉流，保证界面上的人脸框跟随流畅。
  - **慢链路**：`/api/analyze` 负责繁重的音频特征提取、ACRNN 推理和 LLM 生成，处理耗时约 1-2 秒，但不会卡顿前端画面。
- **数据流转**：
  1. 接收前端上传的 `Blob` 音频流和 `text` 文本。
  2. 调用 `MultimodalSystem` 获取融合后的情感向量 $E_{fused}$。
  3. 将 $E_{fused}$ 与用户文本打包，构建 Prompt。
  4. 请求 LLM 服务获取回复。
  5. 将所有中间结果（包括各模态原始分数）打包返回前端，用于图表渲染。

### 6.3 性能优化与体验细节

- **静默加载**：为了提升用户体验，在后端屏蔽了 TensorFlow 和 Transformers 的冗长初始化日志，确保控制台输出干净清晰，便于调试。
- **状态管理**：前端使用 Vue 3 的 `ref` 响应式变量管理 `workflowState`（IDLE -> RECORDING -> PROCESSING），配合 CSS 动画（Loading Spinner），让用户在等待 AI 回复的 1-2 秒空窗期内有明确的预期，消除“系统是否卡死”的担忧。



---



## 7. 总结与展望

- **8.1 总结**：完成了一个从底层模型训练到上层应用开发的全栈 AI 项目。
- **8.2 改进**：预期未来还能与时序上下文建模与LLM协同决策等SOTA技术进行结合。
- **8.3 展望**：应用于具身智能等领域。