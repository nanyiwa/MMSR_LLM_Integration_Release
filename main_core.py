import os
import sys
import time
import json
import torch

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ç³»ç»Ÿè·¯å¾„ä¸­
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.append(project_root)

# 1. å¯¼å…¥ä¸‰å¤§æ¨¡æ€é¢„æµ‹å™¨
from src.modules.audio.inference import EmotionPredictor as AudioModel
from src.modules.text.inference import TextPredictor as TextModel
from src.modules.vision.inference import VisionPredictor as VisionModel
from src.fusion.weighted_fusion import WeightedFusion


class MultimodalSystem:
    def __init__(self):
        print("\nğŸš€ [System] æ­£åœ¨åˆå§‹åŒ–å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ ¸å¿ƒ (çº¯å‡€ç‰ˆ)...")
        start_time = time.time()

        # --- åŠ è½½æ¨¡æ€æ¨¡å‹ ---
        try:
            self.audio_engine = AudioModel()
        except Exception as e:
            print(f"âš ï¸ éŸ³é¢‘æ¨¡å—åŠ è½½å¤±è´¥: {e}")
            self.audio_engine = None

        try:
            self.text_engine = TextModel()
        except Exception as e:
            print(f"âš ï¸ æ–‡æœ¬æ¨¡å—åŠ è½½å¤±è´¥: {e}")
            self.text_engine = None

        try:
            self.vision_engine = VisionModel()
        except Exception as e:
            print(f"âš ï¸ è§†è§‰æ¨¡å—åŠ è½½å¤±è´¥: {e}")
            self.vision_engine = None

        self.fusion_engine = WeightedFusion()

        print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ! è€—æ—¶: {time.time() - start_time:.2f}s\n")

    def analyze(self, audio_path=None, text_content=None, image_path=None):
        """
        å…¨æµç¨‹åˆ†æ: å•æ¨¡æ€ -> åŠ æƒèåˆ (ä¸å†ç»è¿‡ LLM åˆ¤å†³)
        """
        # --- Step 1: å•æ¨¡æ€æ¨ç† ---
        audio_res = {}
        text_res = {}
        vision_res = {}

        if self.audio_engine and audio_path:
            audio_res = self.audio_engine.predict(audio_path)
            if "error" in audio_res: audio_res = {}

        if self.text_engine and text_content:
            text_res = self.text_engine.predict(text_content)

        if self.vision_engine and image_path:
            vision_res = self.vision_engine.predict(image_path)
            if "error" in vision_res: vision_res = {}

        # --- Step 2: åŠ æƒèåˆ ---
        # ä½ çš„æƒé‡ç­–ç•¥
        weights = {'audio': 0.3, 'text': 0.2, 'vision': 0.5}
        fused_result = self.fusion_engine.fuse(audio_res, text_res, vision_res, weights)

        # ç¡®ä¿ fused_result æ˜¯æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åºçš„
        # è¿™æ ·æˆ‘ä»¬å¯ä»¥ç›´æ¥å–ç¬¬ä¸€ä¸ªä½œä¸ºæœ€ç»ˆç»“æœ
        if fused_result:
            fused_result = dict(sorted(fused_result.items(), key=lambda x: x[1], reverse=True))
            # ã€å…³é”®ä¿®æ”¹ã€‘æœ€ç»ˆå†³å®šç›´æ¥å–èåˆåˆ†æ•°çš„ No.1
            top_emotion = list(fused_result.keys())[0]
        else:
            top_emotion = "neutral"

        # --- Step 3: æ„é€ è¿”å›åŒ… ---
        response = {
            "final_decision": top_emotion,
            "details": {
                "fused_scores": fused_result,
                "audio": audio_res,
                "text": text_res,
                "vision": vision_res
            }
        }
        return response

if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    core = MultimodalSystem()
    print("æ ¸å¿ƒå·²å¯åŠ¨ï¼Œç­‰å¾…è°ƒç”¨...")